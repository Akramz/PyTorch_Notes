{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression the PyTorch Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Rhythm\n",
    "1. Design your Model using class with Variables.\n",
    "2. Construct Loss and Optimizer (Select from PyTorch API).\n",
    "3. Training Cycle (Forward, Backward, Update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to turn the Model:\n",
    "$$\\hat{y}=w*x + b$$\n",
    "Into a ready to train PyTorch Implementation, let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(Tensor([[.1], [.2], [.3]]))\n",
    "y = Variable(Tensor([[.2], [.4], [.6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class Definition\n",
    "A neural network in PyTorch is a class definition that inherits from `nn.Module`.\n",
    "\n",
    "We'll implement it for our Linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    '''\n",
    "    In the Constructor we instantiate two nn.Linear Modules\n",
    "    One in and One out.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=1, out_features=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        In the forward function we accept a variable of Input Data.\n",
    "        And we must return a variable of output data.\n",
    "        We can use modules defined in the constructor as ..\n",
    "        .. Arbitrary operators on Variables.\n",
    "        '''\n",
    "        y_hat = self.relu(self.linear(x))\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__` serves as a step for initialization and defining some elements and methods that will be used class-wise, the logic follow general OOP Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use our neural network like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Optimizer Definition\n",
    "We'll use predefined methods in PyTorch to declare the Loss and optimizer functions and add them to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Call to `model.parameters()` in the SGD constructor will contain the learnable weights of the Model, we pass the parameters that we want to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training: Forward, Loss, Backward, Step Update\n",
    "\n",
    "Now we'll create the Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14909563958644867  | Y_hat:  [0.44804713 0.37653    0.30501288]  / Y:  [0.2 0.4 0.6]  / Weights:  [-0.71517134, 0.5195643]\n",
      "500 0.1000080555677414  | Y_hat:  [0.4264722  0.4028843  0.37929642]  / Y:  [0.2 0.4 0.6]  / Weights:  [-0.23587887, 0.45006007]\n",
      "1000 0.0680781826376915  | Y_hat:  [0.38685352 0.4023797  0.41790587]  / Y:  [0.2 0.4 0.6]  / Weights:  [0.15526173, 0.37132734]\n",
      "1500 0.04634266346693039  | Y_hat:  [0.35416576 0.40196344 0.44976112]  / Y:  [0.2 0.4 0.6]  / Weights:  [0.4779768, 0.30636808]\n",
      "2000 0.03154672309756279  | Y_hat:  [0.3271963  0.40161994 0.47604358]  / Y:  [0.2 0.4 0.6]  / Weights:  [0.7442365, 0.25277263]\n",
      "2500 0.021474670618772507  | Y_hat:  [0.30494475 0.40133658 0.4977284 ]  / Y:  [0.2 0.4 0.6]  / Weights:  [0.96391815, 0.20855294]\n",
      "3000 0.014618408866226673  | Y_hat:  [0.286586   0.40110278 0.5156196 ]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.145168, 0.17206918]\n",
      "3500 0.00995112769305706  | Y_hat:  [0.2714387  0.4009098  0.53038096]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.2947112, 0.14196756]\n",
      "4000 0.00677399430423975  | Y_hat:  [0.25894132 0.40075067 0.54256004]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.4180934, 0.11713198]\n",
      "4500 0.0046112509444355965  | Y_hat:  [0.24863029 0.40061933 0.55260843]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.5198905, 0.09664122]\n",
      "5000 0.003138976637274027  | Y_hat:  [0.24012288 0.40051106 0.56089926]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.6038817, 0.079734705]\n",
      "5500 0.002136790892109275  | Y_hat:  [0.23310383 0.40042162 0.56773937]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.6731778, 0.065786034]\n",
      "6000 0.0014545661397278309  | Y_hat:  [0.22731265 0.40034783 0.57338303]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.7303517, 0.054277472]\n",
      "6500 0.0009901656303554773  | Y_hat:  [0.22253469 0.40028697 0.57803935]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.777523, 0.044782378]\n",
      "7000 0.0006740352837368846  | Y_hat:  [0.21859252 0.4002368  0.58188105]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.8164426, 0.036948267]\n",
      "7500 0.00045883061829954386  | Y_hat:  [0.21533994 0.4001954  0.5850508 ]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.8485544, 0.030484496]\n",
      "8000 0.00031233750632964075  | Y_hat:  [0.2126564  0.4001612  0.58766603]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.875048, 0.025151584]\n",
      "8500 0.00021261644724290818  | Y_hat:  [0.2104423  0.40013298 0.5898237 ]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.896907, 0.020751603]\n",
      "9000 0.00014473537157755345  | Y_hat:  [0.20861557 0.40010974 0.5916039 ]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.9149417, 0.017121399]\n",
      "9500 9.852505172602832e-05  | Y_hat:  [0.2071084  0.40009055 0.5930727 ]  / Y:  [0.2 0.4 0.6]  / Weights:  [1.9298215, 0.014126246]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    # Forward Pass: Compute the predictions.\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Forward Pass: Calculate the Loss.\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    if epoch % 500 == 0: \n",
    "        print(epoch, loss.data[0], ' | Y_hat: ', \n",
    "              str(y_pred.data.numpy().reshape((3,))), \n",
    "              ' / Y: ', str(y.data.numpy().reshape((3,))),\n",
    "             ' / Weights: ', str([w.data.numpy().reshape((1,))[0] for w in model.parameters()])\n",
    "             )\n",
    "    \n",
    "    # zero out the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass.\n",
    "    loss.backward()\n",
    "    \n",
    "    # we update the weights, and the weights live in the optimizer.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the parameters are good, the activation ReLU function does screw up our stuff when we insert negative values, it doesn't have any adjustable parameters, one way to look at this is don't use the ReLU activation function on the last layer when you sometimes expect negative values as predictions. Example of this next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = Variable(Tensor([[-0.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we looped over the points one by one (which is feasable), and updated the weight, but when you have a lot of rows (in the millions for example), it's not efficient, that's why we use Stochastic Gradient Descent, when using SGD we are calculating the loss over a batch of data and not a single point. So it will be much faster that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to test/get prediction of new points, we just call the `forward` method on the model instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training.\n",
    "new_x = Variable(Tensor([[41.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = 41., Y =  79.6376724243164\n"
     ]
    }
   ],
   "source": [
    "# Let's predict.\n",
    "print('X = 41., Y = ', model.forward(new_x).data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exos :\n",
    "<img src=\"Other_optimizers.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Note\n",
    "Before diving in, Let's check if we can visualize pytorch graphs (because torch tracks the graph so it can calcualte the gradients), I've installed a library to do this, let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(m)\n",
    "m = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add some layers.\n",
    "m.add_module('W0', nn.Linear(in_features=8, out_features=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_module('tanh', nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.add_module('W1', nn.Linear(in_features=16, out_features=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Declare the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(1, 8))\n",
    "y = m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"200pt\" height=\"264pt\"\n",
       " viewBox=\"0.00 0.00 199.64 264.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-260 195.6377,-260 195.6377,4 -4,4\"/>\n",
       "<!-- 4707997288 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4707997288</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"151.6172,-20 53.6836,-20 53.6836,0 151.6172,0 151.6172,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.6504\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward1</text>\n",
       "</g>\n",
       "<!-- 4707997456 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4707997456</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"150.1144,-76 55.1864,-76 55.1864,-56 150.1144,-56 150.1144,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.6504\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 4707997456&#45;&gt;4707997288 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4707997456&#45;&gt;4707997288</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.6504,-55.9883C102.6504,-48.9098 102.6504,-39.1714 102.6504,-30.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.1505,-30.3038 102.6504,-20.3039 99.1505,-30.3039 106.1505,-30.3038\"/>\n",
       "</g>\n",
       "<!-- 4707998016 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4707998016</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"154.6289,-132 50.6719,-132 50.6719,-112 154.6289,-112 154.6289,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.6504\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 4707998016&#45;&gt;4707997456 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4707998016&#45;&gt;4707997456</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.6504,-111.9883C102.6504,-104.9098 102.6504,-95.1714 102.6504,-86.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.1505,-86.3038 102.6504,-76.3039 99.1505,-86.3039 106.1505,-86.3038\"/>\n",
       "</g>\n",
       "<!-- 4707998128 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4707998128</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101.4516,-188 -.1508,-188 -.1508,-168 101.4516,-168 101.4516,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.6504\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ExpandBackward</text>\n",
       "</g>\n",
       "<!-- 4707998128&#45;&gt;4707998016 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4707998128&#45;&gt;4707998016</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M59.9469,-167.9883C67.1772,-160.2019 77.3959,-149.1971 86.0244,-139.9049\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.8428,-142.0134 93.0825,-132.3039 83.7132,-137.2502 88.8428,-142.0134\"/>\n",
       "</g>\n",
       "<!-- 4707998296 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4707998296</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"78.3077,-256 22.9931,-256 22.9931,-224 78.3077,-224 78.3077,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.6504\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">W0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.6504\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n",
       "</g>\n",
       "<!-- 4707998296&#45;&gt;4707998128 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4707998296&#45;&gt;4707998128</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.6504,-223.7102C50.6504,-216.0144 50.6504,-206.744 50.6504,-198.5691\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.1505,-198.3512 50.6504,-188.3512 47.1505,-198.3513 54.1505,-198.3512\"/>\n",
       "</g>\n",
       "<!-- 4707998184 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4707998184</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"191.625,-188 119.6758,-188 119.6758,-168 191.625,-168 191.625,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.6504\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 4707998184&#45;&gt;4707998016 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4707998184&#45;&gt;4707998016</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.1751,-167.9883C138.7314,-160.1233 128.1798,-148.9745 119.3301,-139.6239\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.8182,-137.161 112.4023,-132.3039 116.7341,-141.9727 121.8182,-137.161\"/>\n",
       "</g>\n",
       "<!-- 4707998352 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4707998352</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"190.1367,-256 121.1641,-256 121.1641,-224 190.1367,-224 190.1367,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.6504\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">W0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"155.6504\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 8)</text>\n",
       "</g>\n",
       "<!-- 4707998352&#45;&gt;4707998184 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4707998352&#45;&gt;4707998184</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M155.6504,-223.7102C155.6504,-216.0144 155.6504,-206.744 155.6504,-198.5691\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.1505,-198.3512 155.6504,-188.3512 152.1505,-198.3513 159.1505,-198.3512\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1189e58d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(y.mean(), params=dict(m.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very intuitive, this might help in debugging but it's not very good at providing a high level overview of the neural architecture, i'll ignore it until i need it, and by the way, keras doesn't support pytorch so you have to use a high level lib for pytorch if you want to quickly prototype NNs. \n",
    "\n",
    "Let's go back to the course.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
