{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning in machine learning is about finding the weights $W$ that minimizes the cost or loss function:\n",
    "\n",
    "$$arg_{w}min \\ loss(w)$$\n",
    "\n",
    "In most cases, we will have to find thousands if not millions of these weights, so ofcoures we need to figure out a numerical way to look for them.\n",
    "\n",
    "The solution lies in **Gradient Descent**:\n",
    "\n",
    "<img src=\"imgs/Gradient_Descent.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\alpha$: the learning rate (small value like $.01$)\n",
    "* $\\partial{loss} \\over \\partial{w}$: The gradient of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return np.multiply(ws_s[-1],x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the functions are imported from the previous notebook, now we need function that will help us update the weights using this formula:\n",
    "\n",
    "$$w_n \\leftarrow w_{n-1} - {\\partial{loss} \\over \\partial{w}}(w_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, let's calculate ${\\partial{loss} \\over \\partial{w}}(w)$, first we present the loss function, and we go from there:\n",
    "\n",
    "$$J(w)={1 \\over m}\\sum_{i=0}^{m}(\\hat{y_i}-y_i)^{2}$$\n",
    "\n",
    "**We suppose a linear model** of the form $\\hat{y_i}=w \\cdot x_i$ (without supposing a model, we can't calculate the derivative) so:\n",
    "\n",
    "$$\\Rightarrow J(w)={1 \\over m}\\sum_{i=0}^{m}({wx_i}-y_i)^{2}$$\n",
    "\n",
    "$$\\Rightarrow {\\partial{J} \\over \\partial{w}}(w) = {1 \\over m}\\sum_{i=0}^{m}{\\partial{} \\over \\partial{w}}({wx_i}-y_i)^{2}$$\n",
    "\n",
    "$$\\Rightarrow {\\partial{J} \\over \\partial{w}}(w) = {2 \\over m}\\sum_{i=0}^{m}x_i(\\hat{y_i}-y_i)$$\n",
    "\n",
    "Now, let's implement the gradient of the loss in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    '''\n",
    "    Calculates Loss: MSE.\n",
    "    Actual Solution: (y_hat - y)*(y_hat - y).\n",
    "    .. Doesn't support vectors.\n",
    "    '''\n",
    "    y_hat = forward(x)\n",
    "    return sum((np.subtract(y_hat,y))**2)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loss(x, y):\n",
    "    '''\n",
    "    Calculates the Gradient Loss of MSE.\n",
    "    '''\n",
    "    y_hat = forward(x)\n",
    "    return sum(np.multiply(np.subtract(y_hat,y), x))*2/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data.\n",
    "X = [1, 2, 3, 4, 5]\n",
    "y = [8, 16, 24, 32, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights.\n",
    "w_s = np.arange(0, 20, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "gradient_losses = []\n",
    "for w in w_s:\n",
    "    losses.append(loss(X, y))\n",
    "    gradient_losses.append(gradient_loss(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the **Weights vs. Loss** then **Loss vs. Gradient Loss**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_s, losses)\n",
    "\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Weight vs. Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gradient_losses, losses)\n",
    "\n",
    "plt.xlabel('Gradient Loss')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Loss vs. Gradient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of random weights, we will update our weight each time, and calculate the gradient loss and do it again, until the loss is small enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_s = [15]\n",
    "X    = [1, 2, 3, 4, 5]\n",
    "y    = [8, 16, 24, 32, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "losses = []\n",
    "preds = []\n",
    "g_losses = []\n",
    "\n",
    "while True:\n",
    "    # first we calculate the preds.\n",
    "    y_h = forward(X)\n",
    "    preds.append(y_h)\n",
    "    \n",
    "    # we calculate the loss.\n",
    "    l = loss(X, y)\n",
    "    losses.append(l)\n",
    "    \n",
    "    # we break if we have a sufficiently small loss.\n",
    "    if l < 1:\n",
    "        break\n",
    "    \n",
    "    # if not, we calculate the gradient loss.\n",
    "    g_loss = gradient_loss(X, y)\n",
    "    g_losses.append(g_loss)\n",
    "    \n",
    "    # and we update the weight.\n",
    "    new_w = ws_s[-1] - (alpha*g_loss)\n",
    "    ws_s.append(new_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[539.0,\n",
       " 327.92760000000015,\n",
       " 199.51115184000008,\n",
       " 121.38258477945612,\n",
       " 73.84916457982105,\n",
       " 44.929831730363105,\n",
       " 27.33530962475292,\n",
       " 16.630802375699666,\n",
       " 10.11818016537568,\n",
       " 6.155900812614562,\n",
       " 3.7452500543946976,\n",
       " 2.2786101330937285,\n",
       " 1.3863064049742218,\n",
       " 0.8434288167863203]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([15, 30, 45, 60, 75]),\n",
       " array([13.46, 26.92, 40.38, 53.84, 67.3 ]),\n",
       " array([12.2588, 24.5176, 36.7764, 49.0352, 61.294 ]),\n",
       " array([11.321864, 22.643728, 33.965592, 45.287456, 56.60932 ]),\n",
       " array([10.59105392, 21.18210784, 31.77316176, 42.36421568, 52.9552696 ]),\n",
       " array([10.02102206, 20.04204412, 30.06306617, 40.08408823, 50.10511029]),\n",
       " array([ 9.5763972 , 19.15279441, 28.72919161, 38.30558882, 47.88198602]),\n",
       " array([ 9.22958982, 18.45917964, 27.68876946, 36.91835928, 46.1479491 ]),\n",
       " array([ 8.95908006, 17.91816012, 26.87724018, 35.83632024, 44.7954003 ]),\n",
       " array([ 8.74808245, 17.49616489, 26.24424734, 34.99232979, 43.74041223]),\n",
       " array([ 8.58350431, 17.16700862, 25.75051292, 34.33401723, 42.91752154]),\n",
       " array([ 8.45513336, 16.91026672, 25.36540008, 33.82053344, 42.2756668 ]),\n",
       " array([ 8.35500402, 16.71000804, 25.06501206, 33.42001608, 41.77502011]),\n",
       " array([ 8.27690314, 16.55380627, 24.83070941, 33.10761255, 41.38451568])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 13.46,\n",
       " 12.2588,\n",
       " 11.321864000000001,\n",
       " 10.59105392,\n",
       " 10.0210220576,\n",
       " 9.576397204928,\n",
       " 9.22958981984384,\n",
       " 8.959080059478195,\n",
       " 8.748082446392992,\n",
       " 8.583504308186534,\n",
       " 8.455133360385496,\n",
       " 8.355004021100687,\n",
       " 8.276903136458536]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The loss kept getting smaller, the predictions converted to the targets, and the weight converged to $8$, this is the magic of the gradient descent when coupled with a good loss function and a good model (hypothesis space).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of model of 2 parameters, like this one:\n",
    "\n",
    "$$\\hat{y}=x^{2}w_2 + xw_{1} +b$$\n",
    "$$J(w)=(\\hat{y}_w - y)^2$$\n",
    "\n",
    "We need to calculate the partial derivative of the loss function with respect to the 2 parameters, as follows:\n",
    "\n",
    "$${\\partial{J} \\over \\partial{w_1}} = 2x|\\hat{y}_w - y|$$\n",
    "$${\\partial{J} \\over \\partial{w_2}} = 2x^2|\\hat{y}_w - y|$$\n",
    "\n",
    "Then we can update each weight separately as we learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
