{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs: Recurrent Neural Networks\n",
    "A very special kind of neural network that gets the usual input and produces an output but it produces a hidden output that's sometimes called `state` of `hidden state` that get passed on to the next cell:\n",
    "\n",
    "<img src=\"imgs/RNN-unrolled.png\" />\n",
    "\n",
    "The three types of RNN Models are:\n",
    "* One to Many\n",
    "* Many to One\n",
    "* Many to Many\n",
    "\n",
    "RNNs are useful when dealing with sequential data, and have applications in domains such as:\n",
    "* Time Series Prediction\n",
    "* Language Modeling (Text Generation)\n",
    "* Text sentiment Analysis\n",
    "* Named Entity Recognition\n",
    "* Translation\n",
    "* Speech Recognition\n",
    "* Music Composition\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to represent text, to one hot encode them or use embeddings, Let's start with one hot encoding the word 'hello':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's one-hot encode 'hello'.\n",
    "string = 'hello'\n",
    "indices = np.array([i for i in range(len(list(set('hello'))))])\n",
    "letter_to_index = dict(zip(list(set('hello')), indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.zeros((len(string), len(list(set(string)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(one_hot):\n",
    "    one_hot[i][letter_to_index[string[i]]] = 1\n",
    "one_hot = one_hot.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Model that gets 'Hello' and output two random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstRNN(nn.Module):\n",
    "    '''\n",
    "    Our first RNN written using PyTorch, it has one cell.\n",
    "    Input: One-hot encoded 'Hello'\n",
    "    Output: a random vector of size 2.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(FirstRNN, self).__init__()\n",
    "        \n",
    "        # One Cell RNN: Input_dim=4 -> Output_dim=2.\n",
    "        self.cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # we have to initialize the first hidden state.\n",
    "        h = Variable(torch.randn(1,6,2))\n",
    "        \n",
    "        out, hidden = self.cell(x, h)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = FirstRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, h = RNN(Variable(torch.from_numpy(one_hot.reshape((1,5,4)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we want to do this process for A Sequence of size N:\n",
    "\n",
    "<img src=\"imgs/RNN_n_sequence.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output and the hidden state from a give cell are the same, the premise is that that hidden state will be feeded to the next cell to overcome the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to introduce the notion of batches, until now we've dealt with a batch containing one example that contain 5 letters of size 4 (1-hot encoded), so the shape is `(1,5,4)`, next we need to feed in multiple words in the batch, for example 3 words:\n",
    "\n",
    "<img src=\"imgs/RNN_Batch.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_one_hot(string):\n",
    "    '''\n",
    "    Transform a String to a One-hot Encoded Vector.\n",
    "    '''\n",
    "    one_hot = np.zeros((5, 4))\n",
    "    \n",
    "    for i, _ in enumerate(one_hot):\n",
    "        one_hot[i][letter_to_index[string[i]]] = 1\n",
    "\n",
    "    return one_hot.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_one_hot('hhhhh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ['hello', 'hhhhh', 'helll', 'eeehl', 'lhell', 'eeeeh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll transform the batch into the proper data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for word in batch:\n",
    "    b.append(string_to_one_hot(word))\n",
    "b = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a batch of **6** words, each word have **5** characters, each character is a one-hot vector of size **4**.\n",
    "\n",
    "To feed the batch to the `SimpleRNN`, we need to initilize the first hidden state for each Word, After doing that we feed the batch and get the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = RNN(Variable(torch.from_numpy(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Hello' Language Model\n",
    "We are going to teach our RNN to predict the next character:\n",
    "<img src=\"imgs/Hello_LM.png\" />\n",
    "* A Many-to-Many problem\n",
    "* Input/Output dim: 5\n",
    "* This is basically a multi-class classification problem.\n",
    "* As a Loss function we'll choose cross-entropy.\n",
    "* We compute the loss over each letter and we minimize the total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_lookup = [\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = ['h', 'i', 'e', 'l', 'o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = 'hihell'\n",
    "x_data = [0, 1, 0, 2, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = [one_hot_lookup[x] for x in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y: ihello\n",
    "y_data = [1, 0, 2, 3, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we have one batch of samples, we will change them to variables only once.\n",
    "inputs = Variable(torch.Tensor(x_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Variable(torch.LongTensor(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_size = 5  # one-hot size.\n",
    "hidden_size = 5  # Output from the LSTM, 5 to directly predict a one-hot vector corresponding to 1 character. \n",
    "batch_size = 1  # one sentence.\n",
    "sequence_length = 6  # Let's do one by one.\n",
    "num_layers = 1  # one-layer RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SecondRNN, self).__init__()\n",
    "        \n",
    "        # One Cell RNN: Input_dim=4 -> Output_dim=2.\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # reshape input in (batch_size, sequence_length, input_size)\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        \n",
    "        # propagate input through RNN.\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # output is a probability ditribution over the number of classes,\n",
    "        # this is why we reshape the output.\n",
    "        out = out.view(-1, num_classes)\n",
    "        \n",
    "        #return hidden, out\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden and cell states.\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "model = SecondRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will feed in the characters one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we update the weights only after we've calculated the sum of the losses over all character predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 9.799 | prediction: hhholl\n",
      "Epoch: 500 loss: 2.667 | prediction: ihello\n",
      "Epoch: 1000 loss: 2.643 | prediction: ihello\n",
      "Epoch: 1500 loss: 2.634 | prediction: ihello\n",
      "Epoch: 2000 loss: 2.628 | prediction: ihello\n",
      "Epoch: 2500 loss: 2.625 | prediction: ihello\n",
      "Epoch: 3000 loss: 2.622 | prediction: ihello\n",
      "Epoch: 3500 loss: 2.620 | prediction: ihello\n",
      "Epoch: 4000 loss: 2.618 | prediction: ihello\n",
      "Epoch: 4500 loss: 2.617 | prediction: ihello\n",
      "Epoch: 5000 loss: 2.615 | prediction: ihello\n",
      "Epoch: 5500 loss: 2.614 | prediction: ihello\n",
      "Epoch: 6000 loss: 2.613 | prediction: ihello\n",
      "Epoch: 6500 loss: 2.613 | prediction: ihello\n",
      "Epoch: 7000 loss: 2.612 | prediction: ihello\n",
      "Epoch: 7500 loss: 2.611 | prediction: ihello\n",
      "Epoch: 8000 loss: 2.611 | prediction: ihello\n",
      "Epoch: 8500 loss: 2.610 | prediction: ihello\n",
      "Epoch: 9000 loss: 2.610 | prediction: ihello\n",
      "Epoch: 9500 loss: 2.609 | prediction: ihello\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    pred = ''\n",
    "    for inpt, label in zip(inputs, labels):\n",
    "        hidden, output = model(inpt, hidden)\n",
    "        val, idx = output.max(1)\n",
    "        pred += str(idx2char[idx.data[0]])\n",
    "        loss += criterion(output, label)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print('Epoch: ' + str(epoch) + ' loss: %1.3f' % loss.data[0] + ' | prediction: ' + pred)\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same model but with all of the inputs and receive all of the outputs, because the model scales with the batch size with no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 1.716\n",
      "Epoch: 100 loss: 0.510\n",
      "Epoch: 200 loss: 0.520\n",
      "Epoch: 300 loss: 0.508\n",
      "Epoch: 400 loss: 0.507\n",
      "Epoch: 500 loss: 0.507\n",
      "Epoch: 600 loss: 0.507\n",
      "Epoch: 700 loss: 0.507\n",
      "Epoch: 800 loss: 0.507\n",
      "Epoch: 900 loss: 0.507\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    outputs = model(inputs, hidden)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: ' + str(epoch) + ' loss: %1.3f' % loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "<img src=\"imgs/RNN_exercice.png\" />\n",
    "<img src=\"imgs/RNN_second_exo.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Vs. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are more efficient since the feature space is not discrete, and they're trainable and can reflect meaning extracted from the corpus.\n",
    "\n",
    "<img src=\"imgs/one_hot_embeddings.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "<img src=\"imgs/RNN_embedding_exo.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to fully understand the different types of RNNs, Just implement them using PyTorch from scratch (don't use the PyTorch layers), but you can take a look at the source code to compare your code with theirs:\n",
    "* Regular RNN\n",
    "* GRU\n",
    "* LSTM\n",
    "\n",
    "[LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "[Stanford: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
