{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By wide we mean that we can add new features, and by Deep we mean adding new layers, essential doing \"Deep Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigmoid function can squash values, causing the vanishing gradients problem, when you multiply smaller than 1 numbers they will get even smaller, and so on, and when you do backpropagation you will lose gradients. \n",
    "\n",
    "The solution to this problem is to use arbitrary activation functions, especially `ReLU` and it does works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing in the Video was interesting.\n",
    "\n",
    "Exo: **Implement a Wide and Deep NN** yourself using a Real Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
