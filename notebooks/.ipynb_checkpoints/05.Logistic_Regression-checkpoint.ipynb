{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Prediction (0 or 1) is very useful!\n",
    "\n",
    "* Spent $N$ hours for study, pass or fail ?\n",
    "* GPA and GRE scores for the HKUST PHD program, admin or not ?\n",
    "* Soccer game against Japan, win or lose ?\n",
    "* She/He looks good, propose or not ?\n",
    "* ...\n",
    "\n",
    "In these cases, we are facing binary classification problems.\n",
    "\n",
    "A Simple solution to turn a linear model into a binary classification model is to keep the linear hypothesis as is with two parameters but wrap the linear output with the sigmoid function (no parameters), so we are optimizing the same paramaters but the updating will change to make the results closer to one or zero, basically outputting probabilities. here is a visualization:\n",
    "\n",
    "<img src=\"sigmoid.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\hat{y} >= 0.5$ then our final prediction will be $1$, and if else we mark the prediction as $0$ (we do this after training our model, not on training).\n",
    "\n",
    "Now that we've introduced the sigmoid function, we need a new loss function that reflects the classification nature of the problem, that's why we introduce cross entropy (or binary entropy).\n",
    "\n",
    "<img src=\"cross_entropy.png\" />\n",
    "\n",
    "The loss function will be small if the predicted probability is closer to the target, and high if it's far away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(Tensor([[-4.], [-3.], [-2.], [-1.], [0.], [.1], [.2], [.3], [.4], [.5]]))\n",
    "y = Variable(Tensor([[0.], [0.], [0.], [0.], [1], [1], [1], [1], [1], [1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_hat = self.activation(self.linear(x))\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(m)\n",
    "m = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function, optimizer.\n",
    "criterion = nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.RMSprop(params=m.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Y_hat:  [0.1402359  0.21750474 0.3214312  0.44666928 0.57906276 0.59199864\n",
      " 0.6048082  0.6174755  0.62998515 0.6423227 ]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [0.53306484, 0.3189273]\n",
      "10000 Y_hat:  [0.000000e+00 0.000000e+00 3.222511e-25 2.332121e-09 1.000000e+00\n",
      " 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [36.51798, 16.64149]\n",
      "20000 Y_hat:  [0.0000000e+00 0.0000000e+00 2.9510331e-26 7.0474915e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [37.71188, 16.6387]\n",
      "30000 Y_hat:  [0.0000000e+00 0.0000000e+00 1.0223037e-26 4.1515652e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [38.24279, 16.640425]\n",
      "40000 Y_hat:  [0.0000000e+00 0.0000000e+00 5.1320995e-27 2.9450070e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [38.588543, 16.642803]\n",
      "50000 Y_hat:  [0.0000000e+00 0.0000000e+00 3.0830547e-27 2.2819928e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [38.843075, 16.642273]\n",
      "60000 Y_hat:  [0.0000000e+00 0.0000000e+00 2.0423584e-27 1.8527221e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [39.046497, 16.637302]\n",
      "70000 Y_hat:  [0.0000000e+00 0.0000000e+00 1.4730208e-27 1.5793461e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [39.213642, 16.644802]\n",
      "80000 Y_hat:  [0.000000e+00 0.000000e+00 1.083208e-27 1.352846e-10 1.000000e+00\n",
      " 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [39.36623, 16.64259]\n",
      "90000 Y_hat:  [0.0000000e+00 0.0000000e+00 8.5549346e-28 1.1989393e-10 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00]  / Y:  [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]  / Weights:  [39.48146, 16.637047]\n"
     ]
    }
   ],
   "source": [
    "# now we train the shit.\n",
    "for epoch in range(100000):\n",
    "    # Forward Propagation.\n",
    "    y_hat = m.forward(X)\n",
    "    \n",
    "    # calculate the loss.\n",
    "    loss = criterion(y_hat, y)\n",
    "    \n",
    "    if epoch % 10000 == 0: \n",
    "        print(epoch,'Y_hat: ', \n",
    "              str(y_hat.data.numpy().reshape((10,))), \n",
    "              ' / Y: ', str(y.data.numpy().reshape((10,))),\n",
    "             ' / Weights: ', str([w.data.numpy().reshape((1,))[0] for w in m.parameters()])\n",
    "             )\n",
    "\n",
    "    # Back propagation.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
