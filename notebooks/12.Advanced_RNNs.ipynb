{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN II: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical RNN Setup is this:\n",
    "\n",
    "<img src=\"imgs/Typical_RNN.png\" />\n",
    "\n",
    "With:\n",
    "* Based on the task we can use different types of loss functions.\n",
    "* But for this tutorial, A loss function of categorical cross entropy was chosen.\n",
    "* We always chose a gradient-based optimizer algorithm.\n",
    "* The embeddings are also trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we're going to focus on one type of classification, called RNN Classification, where we feedin a sequences of embeddings with the goal of classifying the sequence:\n",
    "\n",
    "<img src=\"imgs/RNN_classification.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input: A sequence of letters (Human Names).\n",
    "* Output: The Language of the Name (18 Unique languages).\n",
    " * Nader -> Arabic\n",
    " * Huie -> Chinese\n",
    " * Zhogin -> Russian\n",
    " \n",
    "<img src=\"imgs/Names_to_language.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use character level embeddings, we will feed the model the ASCII index of each input character, and that index will map to its corresponding embedding vector:\n",
    "\n",
    "<img src=\"imgs/RNN_Model_Architecture.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the ASCII index of a character we use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_ascii(name):\n",
    "    '''\n",
    "    Turns a natural string to its equivalent ASCII.\n",
    "    Input: Name (String).\n",
    "    Output: Input in ASCII, Number of characters.\n",
    "    '''\n",
    "    ASCII_characters = [ord(c) for c in name]\n",
    "    return ASCII_characters, len(ASCII_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "* We have 3 layers:\n",
    " * The Embedding Layer\n",
    " * The RNN GRU Layer\n",
    " * The Dense Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNameClassifier(nn.Module):\n",
    "    '''\n",
    "    An RNN that takes as Input a Person's Name and detects its Language.\n",
    "    Input: String of Characters.\n",
    "    Output: A Probability Distribution over the Corpus Languages (18).\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNNameClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # hidden_size = size of character embedding vectors = size of the hidden state.\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Note: We run this all at once over the whole input sequence.\n",
    "        \n",
    "        # debugging starts here.\n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        # input = B x S . size(0)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding S x B -> S x B x I (embedding size).\n",
    "        # It seems that we need to reshape or view the input before feeding it.\n",
    "        x = x.t()\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Make a hidden.\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # use the last layer output as the fc's input.\n",
    "        # no need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden)\n",
    "        \n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        # each example in the batch_size of inputs needs its initial hidden state.\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 65535  # ASCII ord() possible encodings. \n",
    "EMBEDDING_OUTPUT_SIZE = 100\n",
    "N_CLASSES = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer = RNNameClassifier(input_size=VOCAB_SIZE, hidden_size=EMBEDDING_OUTPUT_SIZE, output_size=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr, _ = name_to_ascii('akram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Variable(torch.LongTensor([arr])); inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifer(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "To feed inputs as batches, we need to add zeros to make them tensors of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create necessary variables, lengths, and target.\n",
    "def make_variables(names):\n",
    "    sequence_and_length = [name_to_ascii(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test feeding a batch of names to the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['adylov', 'solan', 'hard', 'san']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer = RNNameClassifier(input_size=VOCAB_SIZE, hidden_size=EMBEDDING_OUTPUT_SIZE, output_size=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = make_variables(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 18])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(classifer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = criterion(output, target)\n",
    "#loss.backward()\n",
    "#optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll want to use the `torch.nn.utils.rnn.PackedSequence` method to process the packed sequences.\n",
    "\n",
    "<img src=\"imgs/packed_RNNs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to make our operations more efficient is by using GPUs (It's extremely easy to use them in PyTorch):\n",
    "\n",
    "<img src=\"imgs/PyTorch_GPUs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example\n",
    "## Embedding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An embedding module of 5 tensors of size 3.\n",
    "embedding_example = nn.Embedding(num_embeddings=65535, embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = Variable(torch.LongTensor([[1,2,4,5,6]])); example_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr, _ = name_to_ascii('akram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Variable(torch.LongTensor([arr])); inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 100])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_example(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercices\n",
    "\n",
    "<img src=\"imgs/RNN_exo.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
